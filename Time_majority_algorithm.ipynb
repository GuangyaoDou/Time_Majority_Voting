{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from time import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_session_threashold = 0.50\n",
    "data_status = 'old_'\n",
    "data_source = 'rwt'\n",
    "tcr_subject = [7, 112, 113, 121, 75, 107, 79, 82, 118, 76, 115, 117, 119, 120, 105, 78, 124]\n",
    "rwt_subject = [7, 112, 113, 114, 75, 107, 79, 82, 118, 76, 115, 117, 119, 120]\n",
    "new_tcr_trial = [123, 124]\n",
    "subject = None\n",
    "if data_status == \"old_\":\n",
    "    if data_source == 'tcr':\n",
    "        subject = tcr_subject\n",
    "    else:\n",
    "        subject = rwt_subject\n",
    "else:\n",
    "    subject = new_tcr_trial\n",
    "first_chopped_off = 600 * 0.3\n",
    "last_chopped_off = 600 * 0\n",
    "\n",
    "Random_Forest_predicted_y = []\n",
    "RBF_SVM_predicted_y = []\n",
    "\n",
    "\n",
    "GradientBoost = {}\n",
    "NearestNeighbor = {}\n",
    "AdaBoost = {}\n",
    "RandomForest = {}\n",
    "LinearSVM = {}\n",
    "RBFSVM = {}\n",
    "DecisionTree = {}\n",
    "RUSBoost = {}\n",
    "LDA = {}\n",
    "\n",
    "data_left_for_subjects = {} # records the percentage of data being used for each subject\n",
    "\n",
    "time_continuity_subject = {} # records the time of time_continuity_algorithm for each subject\n",
    "\n",
    "subject_accuracy_random_forest_dict = {} # records the accuracy of random forest for each subject\n",
    "\n",
    "subject_accuracy_dict = {} # records the accuracy of time_continuity_algorithm for each subject\n",
    "\n",
    "subject_preprocess_record = {} # records the number of sessions and folds left for each subjects\n",
    "\n",
    "subject_prediction = {}\n",
    "\n",
    "subject_unknown_percentage = {}\n",
    "\n",
    "time_classifier = {} # records the time it takes for each classifier to execute the 7-fold cross-validation\n",
    "# define models to train\n",
    "names = [  \n",
    "#         'GradientBoostingRegressor',\n",
    "#         'Nearest Neighbors',\n",
    "#         'AdaBoostClassifier',\n",
    "        'RandomForest',\n",
    "#         \"Linear SVM\",\n",
    "        \"RBF SVM\",\n",
    "#         \"Decision Tree\",\n",
    "#         'RUSBoost',\n",
    "#         'Shrinkage LDA',\n",
    "        ]\n",
    "\n",
    "# build classifiers\n",
    "classifiers = [\n",
    "#             GradientBoostingRegressor(random_state=1),\n",
    "#             KNeighborsClassifier(n_neighbors=5),\n",
    "#             AdaBoostClassifier(n_estimators=400, learning_rate = 0.6),\n",
    "            RandomForestClassifier(n_estimators=300, max_features = \"sqrt\", oob_score = True),\n",
    "#             SVC(kernel=\"linear\", C=0.025),\n",
    "            SVC(gamma=2, C=1),\n",
    "#             DecisionTreeClassifier(),\n",
    "#             RUSBoostClassifier(n_estimators = 200, random_state=1),\n",
    "#             LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'),\n",
    "              ]\n",
    "\n",
    "dicts_records = [\n",
    "#                  GradientBoost, \n",
    "#                  NearestNeighbor,\n",
    "#                  AdaBoost, \n",
    "                 RandomForest, \n",
    "#                  LinearSVM,\n",
    "                 RBFSVM,\n",
    "#                  DecisionTree,\n",
    "#                  RUSBoost, \n",
    "#                  LDA\n",
    "                ]\n",
    "\n",
    "\n",
    "def check_removed_index(name, removed_dict, index_to_be_removed, lowerBound, upperBound):\n",
    "    if name not in removed_dict:\n",
    "        removed_dict[name] = 0\n",
    "    lst = [i for i in index_to_be_removed if i >= lowerBound and i < upperBound]\n",
    "    removed_dict[name] = removed_dict[name] + len(lst)\n",
    "\n",
    "def calculate_accuracy(y_actual, y_predict):\n",
    "    count = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] == y_predict[i]:\n",
    "            count = count + 1\n",
    "    return count / float(len(y_actual))\n",
    "\n",
    "def check_plateau(dataFrame, current_index):\n",
    "    for i in range(current_index + 1, current_index + 14):\n",
    "        if i >= len(dataFrame):\n",
    "            return True\n",
    "        lst = move_data.iloc[current_index, :].tolist()\n",
    "        if sum(lst) != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def random_output(first, second):\n",
    "    random_num = random.random()\n",
    "    if random_num <= 0.5:\n",
    "        return first\n",
    "    else:\n",
    "        return second\n",
    "\n",
    "print(len(subject))\n",
    "\n",
    "tasks_dict = {} # array of dictionaries\n",
    "for i in range(30):\n",
    "    key_name = \"task\" + str(i+1)\n",
    "#     temp_dict = {}\n",
    "#     temp_dict[key_name] = []\n",
    "#     tasks_dict.append(temp_dict)\n",
    "    tasks_dict[key_name] = []\n",
    "\n",
    "print(tasks_dict)\n",
    "\n",
    "def most_common(List):\n",
    "    return(mode(List))\n",
    "\n",
    "def accuracy_divide(numerator, denominator):\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    return numerator / denominator\n",
    "# subject_2_RF = []\n",
    "# subject_3_RF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517dc346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(1, len(subject) + 1):\n",
    "subject_id_start = 1\n",
    "subject_id_end = 5\n",
    "for i in range(subject_id_start, subject_id_end):\n",
    "    tasks_dict.clear() # array of dictionaries\n",
    "    for task_rank in range(30):\n",
    "        key_name = \"task\" + str(task_rank+1)\n",
    "        tasks_dict[key_name] = []\n",
    "    subject_id = i\n",
    "    print()\n",
    "    print(\"checking subject\",subject_id)\n",
    "    print()\n",
    "    frame = []\n",
    "    for session in range(1,7):\n",
    "        data_one = pd.read_csv('data_preprocess/'+data_status + data_source+'_plateau_removed_data/'+data_source+\"_subject_\"+str(subject_id)+\"_session_\"+str(session)+\".csv\",\n",
    "                      header = None)\n",
    "        zeros = [0] * 20\n",
    "\n",
    "        if len(data_one) <= 3000:\n",
    "            data_one.loc[len(data_one)] = zeros\n",
    "        data_one = data_one.iloc[0:3000]\n",
    "        temp_frame = []\n",
    "        for i in range(5):\n",
    "            temp = data_one.iloc[int(i * 600 + first_chopped_off) : int((i + 1) * 600 - last_chopped_off)]\n",
    "            temp_frame.append(temp)\n",
    "        data_one = pd.concat(temp_frame)\n",
    "        frame.append(data_one)\n",
    "    data = pd.concat(frame)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # check plateau (noise)\n",
    "    index_to_be_removed = []\n",
    "    for session in range(0,6):\n",
    "        temp_data = data.iloc[session * 2100 : (session + 1) * 2100]\n",
    "        for move in range(0,5):\n",
    "            move_data = temp_data.iloc[move * 420 : (move + 1) * 420]\n",
    "#             i = 0\n",
    "#             while i < len(move_data):\n",
    "#                 lst = move_data.iloc[i, :].tolist()\n",
    "#                 temp_lst = []\n",
    "#                 if sum(lst) == 0:\n",
    "#                     if check_plateau(move_data, i):\n",
    "#                         end = i + 14\n",
    "#                         if end >= len(move_data):\n",
    "#                             end = len(move_data)\n",
    "#                         temp_lst = [j for j in range(session * 2100 + move * 420 + i, session * 2100 + move * 420 + end)]\n",
    "#                         i = i + 14\n",
    "#                     else:\n",
    "#                         i = i + 1\n",
    "#                 if len(temp_lst) > 0:\n",
    "#                     index_to_be_removed.extend(temp_lst)\n",
    "            for i in range(len(move_data)):\n",
    "                lst = move_data.iloc[i, :].tolist()\n",
    "                if sum(lst) == 0:\n",
    "                    index_to_be_removed.append(session * 2100 + move * 420 + i)\n",
    "    print(\"number of rows to be removed is\", len(index_to_be_removed))\n",
    "    \n",
    "    # add labels\n",
    "    ones = [1] * int(600 * 0.7)\n",
    "    twos = [2] * int(600 * 0.7)\n",
    "    threes = [3] * int(600 * 0.7)\n",
    "    fours = [4] * int(600 * 0.7)\n",
    "    fives = [5] * int(600 * 0.7)\n",
    "    len(fives)\n",
    "    session1 = ones + twos + threes + fours + fives\n",
    "    session2 = fours + ones + twos + threes + fives\n",
    "    session3 = ones + fours + threes + twos + fives\n",
    "    session4 = ones + twos + threes + fours + fives\n",
    "    session5 = twos + ones + threes + fives + fours\n",
    "    session6 = ones + twos + fours + threes + fives\n",
    "    session_all = session1 + session2 + session3 + session4 + session5 + session6\n",
    "    data[\"ground_truth\"] = session_all\n",
    "    \n",
    "    #check if the subject should be kept\n",
    "    percentage_removed_total = (int(18000 * 0.7) - len(index_to_be_removed)) / 18000.0\n",
    "    print(\"percentage of data left for subject\", subject_id, \"is\", percentage_removed_total)\n",
    "    if percentage_removed_total < 0.35:\n",
    "        print(\"the subject\", subject_id, \"should be removed and will be ignored\")\n",
    "        continue\n",
    "    if str(subject_id) not in subject_unknown_percentage:\n",
    "        subject_unknown_percentage[str(subject_id)] = {}\n",
    "    subject_unknown_percentage[str(subject_id)][\"known\"] = percentage_removed_total \n",
    "    subject_preprocess_record[str(subject_id)] = {}\n",
    "        \n",
    "    # checks each six session:\n",
    "    print(\"check session for subject\", subject_id)\n",
    "    session_list = [i for i in range(0, 6)]\n",
    "    for session in range(0, 6):\n",
    "        session_lowerbound = 2100 * session\n",
    "        session_upperbound = 2100 * (session + 1)\n",
    "        to_be_removed = [i for i in index_to_be_removed if i >= session_lowerbound and i < session_upperbound]\n",
    "        print(\"Number of rows to be removed for session\", (session + 1) , \"is\", len(to_be_removed))\n",
    "        percentage_remained = (2100 - (len(to_be_removed))) / 3000.0\n",
    "        print(\"percent of rows left in sesssion\", (session + 1), \"is\", percentage_remained)\n",
    "        if percentage_remained < 0.35:\n",
    "            session_list.remove(session)\n",
    "            print(\"session \" + str(session + 1) + \" should be removed and will be ignored\")\n",
    "            print()\n",
    "        print()\n",
    "    if len(session_list) == 0:\n",
    "        print(\"all sessions are ignored. Continue to next person\")\n",
    "        continue\n",
    "    \n",
    "    subject_preprocess_record[str(subject_id)][\"session_remained\"] = len(session_list)\n",
    "    \n",
    "    not_session_list = [i for i in range(0, 6) if i not in session_list]\n",
    "    for i in not_session_list:\n",
    "        for j in range(i*5, (i+1)*5):\n",
    "            print(\"j is\",j)\n",
    "            tasks_dict[\"task\"+str(j+1)].extend([-1]*420)\n",
    "#     print(tasks_dict)\n",
    "#     print(\"Test\")\n",
    "#     print(tasks_dict[\"task5\"].count(-1))\n",
    "#     print(\"Test\")\n",
    "    # cut 7 folds\n",
    "    test1data = []\n",
    "    test2data = []\n",
    "    test3data = []\n",
    "    test4data = []\n",
    "    test5data = []\n",
    "    test6data = []\n",
    "    test7data = []\n",
    "    removed_dict = {}\n",
    "    fold_names = [\"fold1\", \"fold2\", \"fold3\", \"fold4\", \"fold5\", \"fold6\", \"fold7\"]\n",
    "    fold_names_dict = {}\n",
    "    for i in range(len(fold_names)):\n",
    "        name = fold_names[i]\n",
    "        fold_names_dict[name] = i + 1\n",
    "#     print(\"the fold_names_dict is\", fold_names_dict)\n",
    "    # for each move (420 lines), split the data into seven folds\n",
    "    # at the same time, record the number of lines being that would be omited\n",
    "    # remove folds that have less than 33.33% data remained\n",
    "    #each fold should have at most 60 * 30 = 1800 (originally 2571.4)\n",
    "    move_lst = []\n",
    "    for ele in session_list:\n",
    "        temp = [i for i in range(ele*5, (ele + 1)*5)]\n",
    "        move_lst.extend(temp)\n",
    "    for i in move_lst:\n",
    "        lowerBound = i * 420\n",
    "        test1data.append(data.iloc[lowerBound : lowerBound + 60])\n",
    "        check_removed_index(\"fold1\", removed_dict, index_to_be_removed, lowerBound, lowerBound + 60)\n",
    "        test2data.append(data.iloc[lowerBound + 60 : lowerBound + 120])\n",
    "        check_removed_index(\"fold2\", removed_dict, index_to_be_removed, lowerBound + 60, lowerBound + 120)\n",
    "        test3data.append(data.iloc[lowerBound + 120 : lowerBound + 180])\n",
    "        check_removed_index(\"fold3\", removed_dict, index_to_be_removed, lowerBound + 120, lowerBound + 180)\n",
    "        test4data.append(data.iloc[lowerBound + 180 : lowerBound + 240])\n",
    "        check_removed_index(\"fold4\", removed_dict, index_to_be_removed, lowerBound + 180, lowerBound + 240)\n",
    "        test5data.append(data.iloc[lowerBound + 240 : lowerBound + 300])\n",
    "        check_removed_index(\"fold5\", removed_dict, index_to_be_removed, lowerBound + 240, lowerBound + 300)\n",
    "        test6data.append(data.iloc[lowerBound + 300 : lowerBound + 360])\n",
    "        check_removed_index(\"fold6\", removed_dict, index_to_be_removed, lowerBound + 300, lowerBound + 360)\n",
    "        test7data.append(data.iloc[lowerBound + 360 : lowerBound + 420])\n",
    "        check_removed_index(\"fold7\", removed_dict, index_to_be_removed, lowerBound + 360, lowerBound + 420)\n",
    "\n",
    "    folds_list = [test1data, test2data, test3data, test4data, test5data, test6data, test7data]\n",
    "    # check folds percentages\n",
    "    for name in fold_names:\n",
    "        removed_num = removed_dict[name]\n",
    "        remained_percentage = (((2100 * len(session_list))/ 7.0) - removed_num) / ((3000 * len(session_list)) / 7.0)\n",
    "        print()\n",
    "        print(\"the \" + name + \" has\", remained_percentage, \"left\")\n",
    "        print()\n",
    "        if remained_percentage < 0.35:\n",
    "            print(name + \" should be removed for subject\", subject_id)\n",
    "            idx = fold_names.index(name)\n",
    "            print(\"the index to be removed is\", idx)\n",
    "            del folds_list[idx]\n",
    "            fold_names.remove(name)\n",
    "    folds = []\n",
    "    print(removed_dict)\n",
    "    print(sum(removed_dict.values()))\n",
    "    for fold in folds_list:\n",
    "        data = pd.concat(fold)\n",
    "        print(data.shape)\n",
    "        folds.append(data)\n",
    "    if len(folds) == 0:\n",
    "        print(\"all folds are ignored. Continue to next person\")\n",
    "        continue\n",
    "    \n",
    "    folds_predicted_lst = []\n",
    "    subject_preprocess_record[str(subject_id)][\"folds_remained\"] = len(folds)\n",
    "    \n",
    "    t_start = time()\n",
    "    subject_prediction[str(subject_id)] = {}\n",
    "    models = zip(names, classifiers, dicts_records)\n",
    "    Random_Forest_predicted_y.clear()\n",
    "    RBF_SVM_predicted_y.clear()\n",
    "    for name, classifier, dicts_record in models:\n",
    "        for key in tasks_dict:\n",
    "            if len(tasks_dict[key]) > 0 and tasks_dict[key].count(-1) == 420:\n",
    "                continue\n",
    "            else:\n",
    "                tasks_dict[key] = []\n",
    "        \n",
    "        print(\"DEBUG after clearing tasks_dict\")\n",
    "        print(tasks_dict)\n",
    "#         tasks_dict.clear() # array of dictionaries\n",
    "#         for i in range(30):\n",
    "#             key_name = \"task\" + str(i+1)\n",
    "#         #     temp_dict = {}\n",
    "#         #     temp_dict[key_name] = []\n",
    "#         #     tasks_dict.append(temp_dict)\n",
    "#             tasks_dict[key_name] = []\n",
    "        folds_predicted_lst.clear()\n",
    "#         if name == \"RandomForest\":\n",
    "#             folds_predicted_lst = Random_Forest_predicted_y\n",
    "#         if name == \"RBF SVM\":\n",
    "#             folds_predicted_lst = RBF_SVM_predicted_y\n",
    "        accuracy = 0\n",
    "        t0 = time()\n",
    "        execute_counter = 1\n",
    "        for i in range(len(folds_list)):\n",
    "            folds.append(folds.pop(0)) # move the first fold to the last, and iterate it 7 times\n",
    "            data = pd.concat(folds[:-1])\n",
    "            X = data.iloc[:, :-1]\n",
    "            y = data.iloc[:, -1]\n",
    "            clf = classifier\n",
    "            clf.fit(X,y)\n",
    "            data_test = folds[-1]\n",
    "            X_test = data_test.iloc[:, :-1]\n",
    "            y_test = data_test.iloc[:, -1]\n",
    "            y_predict = []\n",
    "            if name == \"GradientBoostingRegressor\":\n",
    "                y_predict = clf.predict(X_test)\n",
    "                accuracy = accuracy + clf.score(X_test, y_test)\n",
    "            else:\n",
    "                y_predict = clf.predict(X_test)\n",
    "                folds_predicted_lst.append(y_predict)\n",
    "                print(\"executed\",execute_counter,\"times\")\n",
    "                accuracy = accuracy + calculate_accuracy(y_test.tolist(), y_predict)\n",
    "            execute_counter += 1\n",
    "        t1 = time()\n",
    "        time_elapsed = t1 - t0\n",
    "        print()\n",
    "        print(\"The time it takes to run \" + name + \" is\", time_elapsed)\n",
    "#         if name == 'RandomForest':\n",
    "#             clf_tmp = classifier\n",
    "#             clf_tmp.fit()\n",
    "#             Random_Forest_predicted_y.extend(y_temp_predict)\n",
    "#         if name == 'RBF SVM':\n",
    "#             RBF_SVM_predicted_y.extend(y_predict)\n",
    "        if name not in time_classifier:\n",
    "            time_classifier[name] = 0\n",
    "        time_classifier[name] = time_classifier[name] + time_elapsed\n",
    "        accuracy = accuracy / float(len(folds_list))\n",
    "        subject_prediction[str(subject_id)][name] = {}\n",
    "        subject_prediction[str(subject_id)][name][\"acutual_y\"] = y_test\n",
    "        subject_prediction[str(subject_id)][name][\"predicted_y\"] = y_predict\n",
    "        dicts_record[str(subject_id)] = accuracy\n",
    "        print(\"The accuracy of subject\", subject_id, \"is\", accuracy, \"with the model \" + name)\n",
    "\n",
    "        print()\n",
    "        print(\"length of the folds_predicted_lst for \" + name + \" is\", len(folds_predicted_lst))\n",
    "        print()\n",
    "\n",
    "        fold_new_names = [\"fold1\", \"fold2\", \"fold3\", \"fold4\", \"fold5\", \"fold6\", \"fold7\"]\n",
    "#         print()\n",
    "#         print(\"folds_predicted_lst is\", folds_predicted_lst)\n",
    "#         print()\n",
    "        folds_predicted_lst_counter = 0\n",
    "        for fold_name in fold_new_names:\n",
    "            if fold_name in fold_names:\n",
    "                print(fold_name + \" is in!\")\n",
    "    #             print(\"testing name is\" + name + \" \\n\")\n",
    "                task_counter = 0\n",
    "                temp_lst = folds_predicted_lst[folds_predicted_lst_counter]\n",
    "                for key in tasks_dict:\n",
    "    #                 print(\"testing key is \" + key + \" \\n\")\n",
    "    #                 print(tasks_dict[key])\n",
    "                    if len(tasks_dict[key]) > 0 and tasks_dict[key].count(-1) == 420:\n",
    "                        print(key + \" has been ignored, probably due to the ignored session of the subject\", subject_id)\n",
    "                        continue\n",
    "                    tasks_dict[key].extend(temp_lst[task_counter * 60: (task_counter + 1)*60])\n",
    "#                     print(\"just added 60 elements not ignored\")\n",
    "                    task_counter = task_counter + 1\n",
    "                folds_predicted_lst_counter += 1\n",
    "            else:\n",
    "                print(fold_name + \" is not in!\")\n",
    "                for key in tasks_dict:\n",
    "                    if len(tasks_dict[key]) > 0 and tasks_dict[key].count(-1) == 420:\n",
    "                        print(key + \" has been ignored, probably due to the ignored session of the subject\", subject_id)\n",
    "                        continue\n",
    "                    tasks_dict[key].extend([-1]*60)\n",
    "#                     print(\"just added 60 elements TTTTTT ignored\")\n",
    "            \n",
    "\n",
    "        print()\n",
    "        print(\"Printing tasks_dict\")\n",
    "        count = 1\n",
    "        for key in tasks_dict:\n",
    "            print(str(count) + \" \" + str(len(tasks_dict[key])))\n",
    "            count += 1\n",
    "        print()\n",
    "\n",
    "\n",
    "        if name == \"RandomForest\":\n",
    "            for key in tasks_dict:\n",
    "                Random_Forest_predicted_y.extend(tasks_dict[key])\n",
    "\n",
    "            print()\n",
    "            print(\"The predicted_y length of the \"+ name + \" is\", len(Random_Forest_predicted_y))\n",
    "            print()\n",
    "        if name == \"RBF SVM\":\n",
    "            for key in tasks_dict:\n",
    "                RBF_SVM_predicted_y.extend(tasks_dict[key])\n",
    "\n",
    "            print()\n",
    "            print(\"The predicted_y length of the \"+ name + \" is\", len(RBF_SVM_predicted_y))\n",
    "            print()       \n",
    "        \n",
    "        \n",
    "        print(\"=\"*20)\n",
    "        print()\n",
    "\n",
    "    # Time Majority Voting\n",
    "    task_ground_truth = 1\n",
    "    interval_lst = [(0, 419), (420, 839), (840, 1259), (1260, 1679), (1680, 2099)]\n",
    "\n",
    "    table_data = defaultdict(list)\n",
    "    print(subject_id)\n",
    "    table_data = defaultdict(list)\n",
    "    task_lst = []\n",
    "    task_lst.append([1,2,1,1,2,1]) # task 1\n",
    "    task_lst.append([2,3,4,2,1,2]) # task 2\n",
    "    task_lst.append([3,4,3,3,3,4]) # task 3\n",
    "    task_lst.append([4,1,2,4,5,3]) # task 4\n",
    "    task_lst.append([5,5,5,5,4,5]) # task 5\n",
    "    tasks_idx = [[],[],[],[],[]]\n",
    "    should_be_ignored_lists = [[],[],[],[],[]]\n",
    "\n",
    "    task_count = 1\n",
    "    subject_accuracy_before_numerator = 0\n",
    "    subject_accuracy_before_denominator = 0\n",
    "    subject_accuracy_numerator = 0\n",
    "    subject_accuracy_denominator = 0\n",
    "    \n",
    "    data_used = 0\n",
    "    tmv_result_dict = {}\n",
    "    for task_num, task_idx, ignored_lst in zip(task_lst, tasks_idx, should_be_ignored_lists):\n",
    "        \n",
    "        tmv_result_dict[str(task_count)] = []\n",
    "        for i in range(len(task_num)):\n",
    "            starting = 2100 * i\n",
    "            pos = task_num[i]\n",
    "            interval = interval_lst[pos - 1]\n",
    "            idx_first = starting + interval[0]\n",
    "            idx_second = starting + interval[1]\n",
    "            task_idx.append((idx_first, idx_second))\n",
    "\n",
    "        print(\"Task \" + str(task_count) + \" tasks indices\")\n",
    "        print(task_idx)\n",
    "        print()\n",
    "\n",
    "        session_count = 1\n",
    "        i = 0\n",
    "        length = len(task_idx)\n",
    "        session_map = {}\n",
    "\n",
    "        while i < length:\n",
    "            \n",
    "            interval = task_idx[i]\n",
    "            session_map[interval] = session_count\n",
    "            session = Random_Forest_predicted_y[interval[0] : interval[1]+1]\n",
    "#             print(\"Testing session length is\", len(session))\n",
    "            session = [i for i in session if i != -1]\n",
    "#             print(\"Testing after-session length is\", len(session))\n",
    "            matched_total = len([i for i in session if i == task_ground_truth])\n",
    "            if(len(session) == 0):\n",
    "                session.append(10)\n",
    "            ground_truth_percentage = matched_total / len(session)\n",
    "            print(\"session \" + str(session_count) + \" has percentage to the ground truth: \" + str(ground_truth_percentage))\n",
    "            print()\n",
    "            if ground_truth_percentage < ignore_session_threashold:\n",
    "                print(\"session \" + str(session_count) + \" should be ignored for the task_\" + str(task_count))\n",
    "                print()\n",
    "                ignored_lst.append(interval)\n",
    "            i += 1\n",
    "            session_count += 1\n",
    "\n",
    "        print(\"new task_\" + str(task_count) + \"_idx is\", task_idx)\n",
    "        print()\n",
    "        print(session_map)\n",
    "        print()\n",
    "        print(\"Ignored intervals:\", ignored_lst)\n",
    "        print()\n",
    "\n",
    "        total_accuracy_before_numerator = 0\n",
    "        total_accuracy_before_denominator = 0\n",
    "        total_accuracy_new_numerator = 0\n",
    "        total_accuracy_new_denominator = 0\n",
    "        task_tmp_before = []\n",
    "        task_tmp_new = []\n",
    "        for ele in task_idx: \n",
    "            if ele in ignored_lst:\n",
    "                task_tmp_before.append(-1)\n",
    "                task_tmp_new.append(-1)\n",
    "                tmv_result_dict[str(task_count)].extend([-1]*420)\n",
    "                continue\n",
    "            session_num = session_map[ele]\n",
    "            RF_y = Random_Forest_predicted_y[ele[0] : ele[1] + 1]\n",
    "            RF_y_copy = Random_Forest_predicted_y[ele[0] : ele[1] + 1]\n",
    "#             print(\"RF_y original is\", len(RF_y))\n",
    "            RBF_y = RBF_SVM_predicted_y[ele[0] : ele[1] + 1]\n",
    "            RBF_y_copy = RBF_SVM_predicted_y[ele[0] : ele[1] + 1]\n",
    "            RF_y = [i for i in RF_y if i != -1]\n",
    "            data_used += len(RF_y)\n",
    "#             print(\"RF_y remained is\", len(RF_y))\n",
    "            print()\n",
    "            RBF_y = [j for j in RBF_y if j != -1]\n",
    "#             print(\"RBF_y remained is\", len(RBF_y))\n",
    "#             print()\n",
    "            majority_prediction = most_common(RF_y)\n",
    "#             print(\"majority_prediction of\", session_num, \"is\", majority_prediction)\n",
    "            print()\n",
    "            y_predict = []\n",
    "            for j in range(len(RF_y_copy)):\n",
    "                if RF_y_copy[j] == RBF_y_copy[j]:\n",
    "                    tmv_result_dict[str(task_count)].append(RF_y_copy[j])\n",
    "                else:\n",
    "                    tmv_result_dict[str(task_count)].append(majority_prediction)\n",
    "            for i in range(len(RF_y)):\n",
    "                if RF_y[i] == RBF_y[i]:\n",
    "                    y_predict.append(RF_y[i])\n",
    "                else:\n",
    "                    y_predict.append(majority_prediction)\n",
    "            before_matched_predict = [j for j in RF_y if j == task_ground_truth]\n",
    "            before_accuracy = len(before_matched_predict) / len(RF_y)\n",
    "            task_tmp_before.append(before_accuracy)\n",
    "            total_accuracy_before_numerator += len(before_matched_predict)\n",
    "            total_accuracy_before_denominator += len(RF_y)\n",
    "            print(\"session\", session_num, \"had the prediction accuracy before\", before_accuracy)\n",
    "            print()\n",
    "            matched_predict = [j for j in y_predict if j == task_ground_truth]\n",
    "            accuracy = len(matched_predict) / len(RF_y)\n",
    "            task_tmp_new.append(accuracy)\n",
    "            total_accuracy_new_numerator += len(matched_predict)\n",
    "            total_accuracy_new_denominator += len(RF_y)\n",
    "            print(\"session\", session_num, \"has the prediction accuracy\", accuracy)\n",
    "\n",
    "        total_accuracy_before = accuracy_divide(total_accuracy_before_numerator, total_accuracy_before_denominator)\n",
    "        subject_accuracy_before_numerator += total_accuracy_before_numerator\n",
    "        subject_accuracy_before_denominator += total_accuracy_before_denominator\n",
    "\n",
    "        total_accuracy_new = accuracy_divide(total_accuracy_new_numerator, total_accuracy_new_denominator)\n",
    "#         total_accuracy_new = total_accuracy_new_numerator / total_accuracy_new_denominator\n",
    "#         print(\"DEBUG!\")\n",
    "#         print(\"subject \", subject_id)\n",
    "#         print(\"total_accuracy_new_numerator\", total_accuracy_new_numerator)\n",
    "#         print(\"total_accuracy_new_denominator\", total_accuracy_new_denominator)\n",
    "        subject_accuracy_numerator += total_accuracy_new_numerator\n",
    "        subject_accuracy_denominator += total_accuracy_new_denominator\n",
    "#         print(\"subject_accuracy_numerator\", subject_accuracy_numerator)\n",
    "#         print(\"subject_accuracy_denominator\", subject_accuracy_denominator)\n",
    "\n",
    "        task_tmp_before.append(total_accuracy_before)\n",
    "        task_tmp_new.append(total_accuracy_new)\n",
    "        key_name_before = \"task\"+str(task_count)+\"before\"\n",
    "        key_name_new = \"task\"+str(task_count)+\"new\"\n",
    "        table_data[key_name_before] = task_tmp_before\n",
    "        table_data[key_name_new] = task_tmp_new\n",
    "        task_count = task_count + 1\n",
    "        task_ground_truth += 1\n",
    "        print(\"=\"*30)\n",
    "        print()\n",
    "\n",
    "    t_end = time()\n",
    "    time_elapsed_TC = t_end - t_start \n",
    "    time_continuity_subject[subject_id] = time_elapsed_TC\n",
    "\n",
    "    subject_accuracy_before = accuracy_divide(subject_accuracy_before_numerator, subject_accuracy_before_denominator)\n",
    "#     subject_accuracy_before = subject_accuracy_before_numerator / subject_accuracy_before_denominator\n",
    "    subject_accuracy_random_forest_dict[subject_id] = subject_accuracy_before\n",
    "    print()\n",
    "    print(str(subject_id) + \"'s random forest has accuracy of \" + str(subject_accuracy_before))\n",
    "    print()\n",
    "    subject_accuracy = accuracy_divide(subject_accuracy_numerator, subject_accuracy_denominator)\n",
    "#     subject_accuracy = subject_accuracy_numerator / subject_accuracy_denominator\n",
    "    subject_accuracy_dict[subject_id] = subject_accuracy\n",
    "    print()\n",
    "    print(str(subject_id) + \"'s time continuity algorithm has accuracy of \" + str(subject_accuracy))\n",
    "    print()\n",
    "    print(\"Table data currently is\")\n",
    "    print(table_data)\n",
    "\n",
    "    for key in table_data:\n",
    "        lst = table_data[key]\n",
    "        new_ls = [round(i, 3) for i in lst]\n",
    "        table_data[key] = new_ls\n",
    "\n",
    "    idx_lst = []\n",
    "    for i in range(1, 7):\n",
    "        idx_lst.append(\"session\"+str(i))\n",
    "    idx_lst.append(\"accuracy\")\n",
    "    df = pd.DataFrame(table_data, index=idx_lst)\n",
    "    df.to_csv(\"Time_continuity_results/\"+data_source+\"_subject_\"+str(subject_id)+\"_all_tasks.csv\")\n",
    "\n",
    "    print()\n",
    "    print(str(subject_id) + \" spent \" + str(time_elapsed_TC) + \" execute Time_continuity\")\n",
    "    data_left_for_subjects[subject_id] = data_used / 18000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percentage left for all subjects\", data_left_for_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75182c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Time continuity subject dictionary is\", time_continuity_subject)\n",
    "print()\n",
    "print(\"subject accuracy_RF\", subject_accuracy_random_forest_dict)\n",
    "print()\n",
    "print(\"subject accuracy_TC\", subject_accuracy_dict)\n",
    "\n",
    "\n",
    "# subject_id_order = [10, 12, 13, 15, 8, 17, 14, 2, 3, 5, 9, 1]\n",
    "# subject_id_order = [1,2,3,4]\n",
    "subject_id_order = [z for z in range(subject_id_start, subject_id_end)] \n",
    "subject_id_order.sort(key = lambda x : subject_accuracy_random_forest_dict[x], reverse=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "RF_predict = []\n",
    "New_predict = []\n",
    "percentage_left_subjects = []\n",
    "time_data = []\n",
    "for num in subject_id_order:\n",
    "    RF_predict.append(subject_accuracy_random_forest_dict[num])\n",
    "    New_predict.append(subject_accuracy_dict[num])\n",
    "    percentage_left_subjects.append(data_left_for_subjects[num])\n",
    "    time_data.append(time_continuity_subject[num])\n",
    "    \n",
    "RF_Phase2_accuracy = sum(RF_predict) / len(RF_predict)\n",
    "TMV_accuracy = sum(New_predict) / len(New_predict)\n",
    "print(\"=\"*20)\n",
    "print(\"RF_Phase2_accuracy\", RF_Phase2_accuracy)\n",
    "print(\"TMV_accuracy\", TMV_accuracy)\n",
    "print(\"=\"*20)\n",
    "print()\n",
    "print(\"len of the time data\", len(time_data))\n",
    "X_axis = np.arange(len(subject_id_order))\n",
    "plt.bar(X_axis - 0.2, RF_predict, 0.4, label = 'RF_Phase_2 Accuracy', color = '#FFC0CB')\n",
    "plt.bar(X_axis + 0.2, New_predict, 0.4, label = 'TMV Accuracy', color = '#ADD8E6')\n",
    "plt.plot(X_axis, percentage_left_subjects, marker='D', label = 'Data Remained', color = \"#0343DF\")\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "print(labels)\n",
    "order=[2,1,0]\n",
    "plt.xticks(X_axis, subject_id_order)\n",
    "plt.xlabel('Subject ID orderd by RandomForest Phase 1', fontsize=15)\n",
    "plt.ylabel('Accuracy and Data Remained', fontsize=15)\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order], fontsize=10, loc=0) \n",
    "plt.savefig(\"Time_continuity_results/\"+data_source+\"/Accuracy_all_subjects.jpg\", bbox_inches='tight', dpi = 1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f4437",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TMV_avg_runtime = sum(time_data)/len(time_data)\n",
    "print(\"TMV_avg_runtime\",TMV_avg_runtime)\n",
    "print()\n",
    "plt.figure(figsize=(10, 5))\n",
    "fig, ax1 = plt.subplots()\n",
    "known = []\n",
    "for i in subject_id_order:\n",
    "    known.append(subject_unknown_percentage[str(i)][\"known\"])\n",
    "print(\"known percentage\", known)\n",
    "ax2 = ax1.twinx()\n",
    "lns1 = ax1.plot(X_axis, known, marker='D', label = 'Train Data Remained', color = \"#0343DF\")\n",
    "lns2 = ax2.plot(X_axis, time_data, marker='D', label = 'Runtime', color='#FF796C')\n",
    "ax1.set_ylabel('Data Remained Percentage', fontsize=15)\n",
    "ax2.set_ylabel('Time(s)', fontsize=15)\n",
    "\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=0)\n",
    "ax1.set_xlabel('Subject ID orderd by RandomForest Phase 1', fontsize=15)\n",
    "plt.xticks(X_axis, subject_id_order)\n",
    "plt.savefig(\"Time_continuity_results/\"+data_source+\"/Data_Time.jpg\", bbox_inches='tight', dpi = 1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672fde5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_spent_all_subjects = []\n",
    "id_order_temp = subject_id_order[::-1]\n",
    "for num in id_order_temp:\n",
    "    time_spent_all_subjects.append(time_continuity_subject[num])\n",
    "time_spent_all_subjects = [round(i,3) for i in time_spent_all_subjects]\n",
    "data_time_elapsed = {\"Code Runtime (s)\": time_spent_all_subjects}\n",
    "df = pd.DataFrame(data_time_elapsed, index = id_order_temp)\n",
    "df.to_csv(\"Time_continuity_results/\"+data_source+\"/Runtime_all_subjects.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_left_subjects = [round(j,3) for j in percentage_left_subjects]\n",
    "data_percentage_left = {\"Percent of data left %\": percentage_left_subjects}\n",
    "df_data = pd.DataFrame(data_percentage_left, index = id_order_temp)\n",
    "df_data.to_csv(\"Time_continuity_results/\"+data_source+\"/Data_percentage_left_all_subjects.csv\")\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tmv_result_dict))\n",
    "for key in tmv_result_dict:\n",
    "    print(key)\n",
    "    print(len(tmv_result_dict[key]))\n",
    "# print(tmv_result_dict[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e099c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subject_1_task_1 (or the subject_id_end)\n",
    "subject_id_tmv = subject_id_end - 1\n",
    "interval_session_lst = [(0, 420), (420, 840), (840, 1260), (1260, 1680), (1680, 2100), (2100, 2520)]\n",
    "task_num = 1\n",
    "import matplotlib\n",
    "\n",
    "tmv_predicted = tmv_result_dict[str(task_num)]\n",
    "x_axis = [i + 1 for i in range(420)]\n",
    "x_axis = [i/10 for i in x_axis]\n",
    "# print(\"y_axis is\", y_axis)\n",
    "fig, axs = plt.subplots(6, sharex=True, sharey=True, figsize=(25, 18))\n",
    "\n",
    "\n",
    "y_temp_tmv = tmv_predicted[0 : 420]\n",
    "axs[0].plot(x_axis, y_temp_tmv, label = 'Time Majority Voting')\n",
    "axs[0].set_yticklabels(labels=np.arange(-1,6,1), fontsize=15)\n",
    "axs[0].set_title(\"session 1\", fontsize=20)\n",
    "\n",
    "for i in range(1, len(interval_session_lst)):\n",
    "#     print(i)\n",
    "    interval_session = interval_session_lst[i]\n",
    "#     print(interval_session)\n",
    "    y_temp_tmv = tmv_predicted[interval_session[0] : interval_session[1]]\n",
    "#     print(y_temp_tmv)\n",
    "#     print(\"=\"*20)\n",
    "    axs[i].plot(x_axis, y_temp_tmv)\n",
    "    # axs[1].legend(loc=\"upper right\")\n",
    "    axs[i].set_yticklabels(labels=np.arange(-1,6,1), fontsize=15)\n",
    "    axs[i].set_title(\"session \"+str(i + 1), fontsize=20)\n",
    "\n",
    "plt.xticks(np.arange(0, 43, 2),fontsize=20)\n",
    "plt.yticks(np.arange(-1,6,1), fontsize=15)\n",
    "# plt.xticks(fontsize=15)\n",
    "# matplotlib.rc('ytick', fontsize=15) \n",
    "plt.xlabel('Time Span of Task One for Each Session(s)', fontsize=26)\n",
    "fig.legend(fontsize=23, loc=9)\n",
    "plt.savefig(\"Time_continuity_results/\"+data_source+\"/TMV/subject_\"+str(subject_id_tmv)+\"_task_1.jpg\", dpi = 1000) ## Or it's subject_id_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500eabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap\n",
    "print(\"the length of the tmv_result_dict is\", len(tmv_result_dict))\n",
    "for key in tmv_result_dict:\n",
    "    print(type(key))\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25461965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatmap_data = np.zeros((5, 5))\n",
    "for key in tmv_result_dict:\n",
    "    res = []\n",
    "    y = tmv_result_dict[key]\n",
    "    y = [z for z in y if z != -1]\n",
    "    for j in range(1,6):\n",
    "        total_j = len([i for i in y if i == j])\n",
    "        val = total_j / float(len(y))\n",
    "        res.append(float(val))\n",
    "    heatmap_data[int(key) - 1] = res\n",
    "print(heatmap_data)\n",
    "x_axis_labels = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\"]\n",
    "y_axis_labels = [\"T1:Think\",\"T2:Count\",\"T3:Recall\",\"T4:Breathe\",\"T5:Draw\"]\n",
    "ax = sns.heatmap(heatmap_data, cmap=\"Blues\", vmin= -1, vmax=1, annot=True, fmt=\".2f\", xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.savefig(\"Time_continuity_results/\"+data_source+\"/subejct_\"+str(subject_id_tmv)+\"_TMV_heatmap.jpg\", dpi = 800)\n",
    "plt.title(\"subject_\" + str(subject_id_tmv) + \" all six sessions TMV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
