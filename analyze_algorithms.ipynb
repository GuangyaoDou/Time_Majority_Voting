{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d26b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_status = 'old_'\n",
    "data_source = 'tcr'\n",
    "tcr_subject = [7, 112, 113, 121, 75, 107, 79, 82, 118, 76, 115, 117, 119, 120, 105, 78, 124]\n",
    "rwt_subject = [7, 112, 113, 114, 75, 107, 79, 82, 118, 76, 115, 117, 119, 120]\n",
    "new_tcr_trial = [123, 124]\n",
    "subject = None\n",
    "if data_status == \"old_\":\n",
    "    if data_source == 'tcr':\n",
    "        subject = tcr_subject\n",
    "    else:\n",
    "        subject = rwt_subject\n",
    "else:\n",
    "    subject = new_tcr_trial\n",
    "first_chopped_off = 600 * 0.3\n",
    "last_chopped_off = 600 * 0\n",
    "\n",
    "Random_Forest_predicted_y = []\n",
    "RBF_SVM_predicted_y = []\n",
    "\n",
    "\n",
    "GradientBoost = {}\n",
    "NearestNeighbor = {}\n",
    "AdaBoost = {}\n",
    "RandomForest = {}\n",
    "LinearSVM = {}\n",
    "RBFSVM = {}\n",
    "DecisionTree = {}\n",
    "RUSBoost = {}\n",
    "LDA = {}\n",
    "\n",
    "subject_preprocess_record = {} # records the number of sessions and folds left for each subjects\n",
    "\n",
    "subject_prediction = {}\n",
    "\n",
    "subject_unknown_percentage = {}\n",
    "\n",
    "time_classifier = {} # records the time it takes for each classifier to execute the 7-fold cross-validation\n",
    "# define models to train\n",
    "names = [  \n",
    "        'GradientBoosting',\n",
    "        'KNN',\n",
    "        'AdaBoost',\n",
    "        'RandomForest',\n",
    "        \"Linear SVM\",\n",
    "        \"RBF SVM\",\n",
    "        \"Decision Tree\",\n",
    "        'RUSBoost',\n",
    "        'SLDA',\n",
    "        ]\n",
    "\n",
    "# build classifiers\n",
    "classifiers = [\n",
    "            GradientBoostingRegressor(random_state=1),\n",
    "            KNeighborsClassifier(n_neighbors=5),\n",
    "            AdaBoostClassifier(n_estimators=400, learning_rate = 0.6),\n",
    "            RandomForestClassifier(n_estimators=300, max_features = \"sqrt\", oob_score = True),\n",
    "            SVC(kernel=\"linear\", C=0.025),\n",
    "            SVC(gamma=2, C=1),\n",
    "            DecisionTreeClassifier(),\n",
    "            RUSBoostClassifier(n_estimators = 200, random_state=1),\n",
    "            LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto'),\n",
    "              ]\n",
    "\n",
    "dicts_records = [\n",
    "                 GradientBoost, \n",
    "                 NearestNeighbor,\n",
    "                 AdaBoost, \n",
    "                 RandomForest, \n",
    "                 LinearSVM,\n",
    "                 RBFSVM,\n",
    "                 DecisionTree,\n",
    "                 RUSBoost, \n",
    "                 LDA\n",
    "                ]\n",
    "\n",
    "\n",
    "def check_removed_index(name, removed_dict, index_to_be_removed, lowerBound, upperBound):\n",
    "    if name not in removed_dict:\n",
    "        removed_dict[name] = 0\n",
    "    lst = [i for i in index_to_be_removed if i >= lowerBound and i < upperBound]\n",
    "    removed_dict[name] = removed_dict[name] + len(lst)\n",
    "\n",
    "def calculate_accuracy(y_actual, y_predict):\n",
    "    count = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] == y_predict[i]:\n",
    "            count = count + 1\n",
    "    return count / float(len(y_actual))\n",
    "\n",
    "def check_plateau(dataFrame, current_index):\n",
    "    for i in range(current_index + 1, current_index + 14):\n",
    "        if i >= len(dataFrame):\n",
    "            return True\n",
    "        lst = move_data.iloc[current_index, :].tolist()\n",
    "        if sum(lst) != 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(len(subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(subject) + 1):\n",
    "# for i in range(1, 2):\n",
    "    subject_id = i\n",
    "    print()\n",
    "    print(\"checking subject\",subject_id)\n",
    "    print()\n",
    "    frame = []\n",
    "    for session in range(1,7):\n",
    "        data_one = pd.read_csv('data_preprocess/'+data_status + data_source+'_plateau_removed_data/'+data_source+\"_subject_\"+str(subject_id)+\"_session_\"+str(session)+\".csv\",\n",
    "                      header = None)\n",
    "        zeros = [0] * 20\n",
    "\n",
    "        if len(data_one) <= 3000:\n",
    "            data_one.loc[len(data_one)] = zeros\n",
    "        data_one = data_one.iloc[0:3000]\n",
    "        temp_frame = []\n",
    "        for i in range(5):\n",
    "            temp = data_one.iloc[int(i * 600 + first_chopped_off) : int((i + 1) * 600 - last_chopped_off)]\n",
    "            temp_frame.append(temp)\n",
    "        data_one = pd.concat(temp_frame)\n",
    "        frame.append(data_one)\n",
    "    data = pd.concat(frame)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # check plateau (noise)\n",
    "    index_to_be_removed = []\n",
    "    for session in range(0,6):\n",
    "        temp_data = data.iloc[session * 2100 : (session + 1) * 2100]\n",
    "        for move in range(0,5):\n",
    "            move_data = temp_data.iloc[move * 420 : (move + 1) * 420]\n",
    "#             i = 0\n",
    "#             while i < len(move_data):\n",
    "#                 lst = move_data.iloc[i, :].tolist()\n",
    "#                 temp_lst = []\n",
    "#                 if sum(lst) == 0:\n",
    "#                     if check_plateau(move_data, i):\n",
    "#                         end = i + 14\n",
    "#                         if end >= len(move_data):\n",
    "#                             end = len(move_data)\n",
    "#                         temp_lst = [j for j in range(session * 2100 + move * 420 + i, session * 2100 + move * 420 + end)]\n",
    "#                         i = i + 14\n",
    "#                     else:\n",
    "#                         i = i + 1\n",
    "#                 if len(temp_lst) > 0:\n",
    "#                     index_to_be_removed.extend(temp_lst)\n",
    "            for i in range(len(move_data)):\n",
    "                lst = move_data.iloc[i, :].tolist()\n",
    "                if sum(lst) == 0:\n",
    "                    index_to_be_removed.append(session * 2100 + move * 420 + i)\n",
    "    print(\"number of rows to be removed is\", len(index_to_be_removed))\n",
    "    \n",
    "    # add labels\n",
    "    ones = [1] * int(600 * 0.7)\n",
    "    twos = [2] * int(600 * 0.7)\n",
    "    threes = [3] * int(600 * 0.7)\n",
    "    fours = [4] * int(600 * 0.7)\n",
    "    fives = [5] * int(600 * 0.7)\n",
    "    len(fives)\n",
    "    session1 = ones + twos + threes + fours + fives\n",
    "    session2 = fours + ones + twos + threes + fives\n",
    "    session3 = ones + fours + threes + twos + fives\n",
    "    session4 = ones + twos + threes + fours + fives\n",
    "    session5 = twos + ones + threes + fives + fours\n",
    "    session6 = ones + twos + fours + threes + fives\n",
    "    session_all = session1 + session2 + session3 + session4 + session5 + session6\n",
    "    data[\"ground_truth\"] = session_all\n",
    "    \n",
    "    #check if the subject should be kept\n",
    "    percentage_removed_total = (int(18000 * 0.7) - len(index_to_be_removed)) / 18000.0\n",
    "    print(\"percentage of data left for subject\", subject_id, \"is\", percentage_removed_total)\n",
    "    if percentage_removed_total < 0.35:\n",
    "        print(\"the subject\", subject_id, \"should be removed and will be ignored\")\n",
    "        continue\n",
    "    if str(subject_id) not in subject_unknown_percentage:\n",
    "        subject_unknown_percentage[str(subject_id)] = {}\n",
    "    subject_unknown_percentage[str(subject_id)][\"known\"] = percentage_removed_total \n",
    "    subject_preprocess_record[str(subject_id)] = {}\n",
    "        \n",
    "    # checks each six session:\n",
    "    print(\"check session for subject\", subject_id)\n",
    "    session_list = [i for i in range(0, 6)]\n",
    "    for session in range(0, 6):\n",
    "        session_lowerbound = 2100 * session\n",
    "        session_upperbound = 2100 * (session + 1)\n",
    "        to_be_removed = [i for i in index_to_be_removed if i >= session_lowerbound and i < session_upperbound]\n",
    "        print(\"Number of rows to be removed for session\", (session + 1) , \"is\", len(to_be_removed))\n",
    "        percentage_remained = (2100 - (len(to_be_removed))) / 3000.0\n",
    "        print(\"percent of rows left in sesssion\", (session + 1), \"is\", percentage_remained)\n",
    "        if percentage_remained < 0.35:\n",
    "            session_list.remove(session)\n",
    "            print(\"session\", session, \" should be removed and will be ignored\")\n",
    "            print()\n",
    "        print()\n",
    "    if len(session_list) == 0:\n",
    "        print(\"all sessions are ignored. Continue to next person\")\n",
    "        continue\n",
    "    \n",
    "    subject_preprocess_record[str(subject_id)][\"session_remained\"] = len(session_list)\n",
    "    \n",
    "    \n",
    "    # cut 7 folds\n",
    "    test1data = [];\n",
    "    test2data = [];\n",
    "    test3data = [];\n",
    "    test4data = [];\n",
    "    test5data = [];\n",
    "    test6data = [];\n",
    "    test7data = [];\n",
    "    removed_dict = {}\n",
    "    fold_names = [\"fold1\", \"fold2\", \"fold3\", \"fold4\", \"fold5\", \"fold6\", \"fold7\"]\n",
    "    # for each move (420 lines), split the data into seven folds\n",
    "    # at the same time, record the number of lines being that would be omited\n",
    "    # remove folds that have less than 33.33% data remained\n",
    "    #each fold should have at most 60 * 30 = 1800 (originally 2571.4)\n",
    "    move_lst = []\n",
    "    for ele in session_list:\n",
    "        temp = [i for i in range(ele*5, (ele + 1)*5)]\n",
    "        move_lst.extend(temp)\n",
    "    for i in move_lst:\n",
    "        lowerBound = i * 420\n",
    "        test1data.append(data.iloc[lowerBound : lowerBound + 60])\n",
    "        check_removed_index(\"fold1\", removed_dict, index_to_be_removed, lowerBound, lowerBound + 60)\n",
    "        test2data.append(data.iloc[lowerBound + 60 : lowerBound + 120])\n",
    "        check_removed_index(\"fold2\", removed_dict, index_to_be_removed, lowerBound + 60, lowerBound + 120)\n",
    "        test3data.append(data.iloc[lowerBound + 120 : lowerBound + 180])\n",
    "        check_removed_index(\"fold3\", removed_dict, index_to_be_removed, lowerBound + 120, lowerBound + 180)\n",
    "        test4data.append(data.iloc[lowerBound + 180 : lowerBound + 240])\n",
    "        check_removed_index(\"fold4\", removed_dict, index_to_be_removed, lowerBound + 180, lowerBound + 240)\n",
    "        test5data.append(data.iloc[lowerBound + 240 : lowerBound + 300])\n",
    "        check_removed_index(\"fold5\", removed_dict, index_to_be_removed, lowerBound + 240, lowerBound + 300)\n",
    "        test6data.append(data.iloc[lowerBound + 300 : lowerBound + 360])\n",
    "        check_removed_index(\"fold6\", removed_dict, index_to_be_removed, lowerBound + 300, lowerBound + 360)\n",
    "        test7data.append(data.iloc[lowerBound + 360 : lowerBound + 420])\n",
    "        check_removed_index(\"fold7\", removed_dict, index_to_be_removed, lowerBound + 360, lowerBound + 420)\n",
    "\n",
    "    folds_list = [test1data, test2data, test3data, test4data, test5data, test6data, test7data]\n",
    "    # check folds percentages\n",
    "    for name in fold_names:\n",
    "        removed_num = removed_dict[name]\n",
    "        remained_percentage = (((2100 * len(session_list))/ 7.0) - removed_num) / ((3000 * len(session_list)) / 7.0)\n",
    "        print()\n",
    "        print(\"the \" + name + \" has\", remained_percentage, \"left\")\n",
    "        print()\n",
    "        if remained_percentage < 0.35:\n",
    "            print(name + \" should be removed for subject\", subject_id)\n",
    "            idx = fold_names.index(name)\n",
    "            print(\"the index to be removed is\", idx)\n",
    "            del folds_list[idx]\n",
    "            fold_names.remove(name)\n",
    "    folds = []\n",
    "    print(removed_dict)\n",
    "    print(sum(removed_dict.values()))\n",
    "    for fold in folds_list:\n",
    "        data = pd.concat(fold)\n",
    "        print(data.shape)\n",
    "        folds.append(data)\n",
    "    if len(folds) == 0:\n",
    "        print(\"all folds are ignored. Continue to next person\")\n",
    "        continue\n",
    "    \n",
    "    subject_preprocess_record[str(subject_id)][\"folds_remained\"] = len(folds)\n",
    "    \n",
    "    subject_prediction[str(subject_id)] = {}\n",
    "    models = zip(names, classifiers, dicts_records)\n",
    "    for name, classifier, dicts_record in models:\n",
    "        accuracy = 0\n",
    "        t0 = time()\n",
    "        for i in range(len(folds_list)):\n",
    "            folds.append(folds.pop(0))\n",
    "            data = pd.concat(folds[:-1])\n",
    "            X = data.iloc[:, :-1]\n",
    "            y = data.iloc[:, -1]\n",
    "            clf = classifier\n",
    "            clf.fit(X,y)\n",
    "            data_test = folds[-1]\n",
    "            X_test = data_test.iloc[:, :-1]\n",
    "            y_test = data_test.iloc[:, -1]\n",
    "            y_predict = []\n",
    "            if name == \"GradientBoosting\":\n",
    "                y_predict = clf.predict(X_test)\n",
    "                accuracy = accuracy + clf.score(X_test, y_test)\n",
    "            else:\n",
    "                y_predict = clf.predict(X_test)\n",
    "                if name == 'RandomForest':\n",
    "                    Random_Forest_predicted_y.extend(y_predict)\n",
    "                if name == 'RBF SVM':\n",
    "                    RBF_SVM_predicted_y.extend(y_predict)\n",
    "                accuracy = accuracy + calculate_accuracy(y_test.tolist(), y_predict)\n",
    "        t1 = time()\n",
    "        time_elapsed = t1 - t0\n",
    "        print()\n",
    "        print(\"The time it takes to run \" + name + \" is\", time_elapsed)\n",
    "        if name not in time_classifier:\n",
    "            time_classifier[name] = 0\n",
    "        time_classifier[name] = time_classifier[name] + time_elapsed\n",
    "        accuracy = accuracy / float(len(folds_list))\n",
    "        subject_prediction[str(subject_id)][name] = {}\n",
    "        subject_prediction[str(subject_id)][name][\"acutual_y\"] = y_test\n",
    "        subject_prediction[str(subject_id)][name][\"predicted_y\"] = y_predict\n",
    "        dicts_record[str(subject_id)] = accuracy\n",
    "        print(\"The accuracy of subject\", subject_id, \"is\", accuracy, \"with the model \" + name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ffd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dicts_order is:\")\n",
    "for name, dicts_record in zip(names, dicts_records):\n",
    "    print(name)\n",
    "    print(dicts_record)\n",
    "    print()\n",
    "        \n",
    "total = 0\n",
    "best_classifier_name = \"\"\n",
    "dict_sum_recorder = {}\n",
    "for name, dicts_record in zip(names, dicts_records):\n",
    "    cur = sum(dicts_record.values())\n",
    "    dict_sum_recorder[name] = cur\n",
    "    if cur > total:\n",
    "        total = cur\n",
    "        best_classifier_name = name\n",
    "best_classifier = classifiers[names.index(best_classifier_name)]\n",
    "best_classifier_dict = dicts_records[names.index(best_classifier_name)]\n",
    "print(\"The best classifier is: \" + best_classifier_name)\n",
    "print(\"the dictionary for the best classifier is: \")\n",
    "print(best_classifier_dict)\n",
    "print()\n",
    "\n",
    "dict_sum_recorder = dict(sorted(dict_sum_recorder.items(), key=lambda item: -item[1]))\n",
    "print(\"The dic_sum_recorder is\")\n",
    "print(dict_sum_recorder)\n",
    "print()\n",
    "classifier_order = list(dict_sum_recorder.keys())\n",
    "print(\"the order of the classifier is: \")\n",
    "print(classifier_order)\n",
    "print()\n",
    "best_classifier_dict_sorted = dict(sorted(best_classifier_dict.items(), key=lambda item: -item[1]))\n",
    "subject_id_order = list(best_classifier_dict_sorted.keys()) # The x axis of the plot\n",
    "print(\"best_classifier_dict_sorted is: \")\n",
    "print(best_classifier_dict_sorted)\n",
    "print()\n",
    "print(\"The order of the subject id is\")\n",
    "print(subject_id_order)\n",
    "print()\n",
    "\n",
    "result_y_res = [] # each element follows the order of the classifier_order\n",
    "for i in range(len(classifier_order)):\n",
    "    temp_lst = []\n",
    "    classifier = classifier_order[i]\n",
    "    for subject in subject_id_order:\n",
    "        idx = names.index(classifier)\n",
    "        temp_lst.append(dicts_records[idx][subject])\n",
    "    result_y_res.append(temp_lst)\n",
    "\n",
    "print(result_y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184760f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_order.reverse()\n",
    "x_axis = subject_id_order\n",
    "\n",
    "print(x_axis)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(result_y_res)):\n",
    "    result_y_res[i].reverse()\n",
    "    y = result_y_res[i]\n",
    "    label_name = classifier_order[i]\n",
    "    temp_avg = dict_sum_recorder[label_name] / float(len(subject_id_order))\n",
    "    temp_avg = round(temp_avg, 2)\n",
    "    ax.plot(x_axis, y, marker='D', label = label_name + \" (\" + str(temp_avg)+\")\")\n",
    "\n",
    "ax.set_position([0.1,0.5, 1.2, 1.0])\n",
    "ax.legend(loc='upper left')\n",
    "plt.axhline(y=0.2, color='r', linestyle=':')\n",
    "plt.xlabel('Subject ID orderd by ' + best_classifier_name, fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "# plt.figure(figsize=(5, 5))\n",
    "plt.savefig(data_source + \"_\" +data_status+\"results/algorithm_comparison_each_subject.jpg\", bbox_inches='tight', dpi = 2000)\n",
    "plt.show()\n",
    "# 0.2 dot line : random: done\n",
    "# remove gradientBoosting: done\n",
    "\n",
    "# if time allows, do voting : https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py\n",
    "# (follow csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = np.zeros((5, 5))\n",
    "clf = classifiers[names.index(best_classifier_name)]\n",
    "subject_id = subject_id_order[-1] # last person in the list has the highest accuracy\n",
    "y = subject_prediction[str(subject_id)][best_classifier_name][\"acutual_y\"].tolist()\n",
    "predict_y = subject_prediction[str(subject_id)][best_classifier_name][\"predicted_y\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for i in range(1,6):\n",
    "    res = []\n",
    "    test_msk = []\n",
    "    for idx in range(len(y)):\n",
    "        if y[idx] == i:\n",
    "            test_msk.append(idx)\n",
    "    for j in range(1,6):\n",
    "        total_j = 0\n",
    "        for idx in test_msk:\n",
    "            if predict_y[idx] == j:\n",
    "                total_j = total_j + 1\n",
    "        value = total_j / float(len(test_msk))\n",
    "        res.append(value)\n",
    "    heatmap_data[i - 1] = res\n",
    "print(heatmap_data)\n",
    "x_axis_labels = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\"]\n",
    "y_axis_labels = [\"T1:Think\",\"T2:Count\",\"T3:Recall\",\"T4:Breathe\",\"T5:Draw\"]\n",
    "ax = sns.heatmap(heatmap_data, cmap=\"Blues\", vmin= -1, vmax=1, annot=True, fmt=\".2f\", xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.savefig(data_source + \"_\" +data_status+\"results/best_subject_heapmap.jpg\", dpi = 2000)\n",
    "plt.title(\"The heatmap for subject_\" + str(subject_id) + \" all six sessions\")\n",
    "\n",
    "# remove extra digits (两位小数点) (论文最后): not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf672d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw accuracy unknown known graph\n",
    "print(\"subject_unknown_percentage is\", subject_unknown_percentage)\n",
    "noise = []\n",
    "known = []\n",
    "for i in subject_id_order:\n",
    "    known.append(subject_unknown_percentage[str(i)][\"known\"])\n",
    "print(\"known percentage\", known)\n",
    "\n",
    "noise = [(1 - i) for i in known]\n",
    "print(\"noise percentage\", noise)\n",
    "\n",
    "X_axis = np.arange(len(subject_id_order))\n",
    "plt.bar(X_axis - 0.2, noise, 0.4, label = 'noise', color = 'lightcoral')\n",
    "plt.bar(X_axis + 0.2, known, 0.4, label = 'known', color = 'cornflowerblue')\n",
    "plt.xticks(X_axis, subject_id_order)\n",
    "y = []\n",
    "for i in subject_id_order:\n",
    "    y.append(best_classifier_dict[str(i)])\n",
    "plt.plot(subject_id_order, y, marker='D', color=\"purple\")\n",
    "plt.xlabel(\"subjects sorted by accuracy\")\n",
    "# plt.xlabel(\"trials sorted by accuracy\")\n",
    "plt.ylabel(\"accuracy and data percentage\")\n",
    "# plt.title(best_classifier_name + \" accuracy and data distribution\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig(data_source + \"_\" +data_status+\"results/best_classifier_data_distribution.jpg\", dpi = 2000)\n",
    "plt.show()\n",
    "\n",
    "# match color to 958 (论文第二个位置) : done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record algorithm running time (new graph 3 rows: first row: mean accuracy)\n",
    "#     accuracy code_runtime(s) (排序 based on accuracy)\n",
    "# rf 0.56     15\n",
    " \n",
    "# lda 0.4     xx\n",
    "\n",
    "# adaboost 0.3 xx\n",
    "avg_accuracy = []\n",
    "time = []\n",
    "name_list = []\n",
    "print(\"The time_classifier is\", time_classifier)\n",
    "print(\"The dict_sum_recorder is\", dict_sum_recorder)\n",
    "print(\"Number of subjects is\", len(subject_id_order))\n",
    "for ele in dict_sum_recorder:\n",
    "    name_list.append(ele)\n",
    "    temp_avg = dict_sum_recorder[ele] / float(len(subject_id_order))\n",
    "    avg_accuracy.append(round(temp_avg, 2))\n",
    "    temp_time = time_classifier[ele] / float(len(subject_id_order))\n",
    "    time.append(round(temp_time, 1))\n",
    "\n",
    "print()\n",
    "print(\"avg accuracy\", avg_accuracy)\n",
    "print(\"time\", time)\n",
    "print(\"name order\", name_list)\n",
    "\n",
    "data = {'Average Accuracy':avg_accuracy, 'Avg code runtime(s)':time}\n",
    "# Creates pandas DataFrame.  \n",
    "df = pd.DataFrame(data, index = name_list)\n",
    "df.to_csv(data_source + \"_\" +data_status+\"results/accuracy_runtime_classifier.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject_preprocess_record)\n",
    "\n",
    "print(x_axis)\n",
    "x_axis.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_removed = []\n",
    "folds_removed = []\n",
    "\n",
    "for i in x_axis:\n",
    "    data_dict = subject_preprocess_record[i]\n",
    "    session_removed.append(6 - data_dict[\"session_remained\"])\n",
    "    folds_removed.append(7 - data_dict['folds_remained'])\n",
    "print(session_removed)\n",
    "print(folds_removed)\n",
    "data_session_folds = {'session removed': session_removed, \"folds removed\": folds_removed}\n",
    "df = pd.DataFrame(data_session_folds, index = x_axis)\n",
    "df.to_csv(data_source + \"_\" +data_status+\"results/subjects_removed.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d475ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import sklearn\n",
    "print(python_version())\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
